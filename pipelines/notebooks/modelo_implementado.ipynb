{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JUANITOsvg/proyecto_ds_jueves/blob/main/pipelines/notebooks/modelo_implementado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6vnnzzUZteT"
      },
      "outputs": [],
      "source": [
        "# Formula 1 Race Prediction Models\n",
        "# This notebook implements two ML models for F1 race data analysis\n",
        "\n",
        "print(\"Starting F1 ML Model Development...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Formula 1 Machine Learning Models**\n",
        "\n",
        "This notebook implements two complex ML models using multiple F1 datasets:\n",
        "\n",
        "1. **Race Result Predictor**: Predicts podium finishes (top 3 positions)\n",
        "2. **Driver Performance Classifier**: Classifies drivers into performance tiers\n",
        "\n",
        "## Dataset Overview\n",
        "We'll use multiple interconnected F1 datasets:\n",
        "- **races.csv**: Race information (circuits, dates, years)\n",
        "- **results.csv**: Race results and performance data\n",
        "- **drivers.csv**: Driver information and demographics\n",
        "- **constructors.csv**: Team/constructor data\n",
        "- **qualifying.csv**: Qualifying session results\n",
        "- **lap_times.csv**: Detailed lap timing data\n",
        "- **pit_stops.csv**: Pit stop strategy data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Analysis and Visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Additional utilities\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Initial Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all F1 datasets\n",
        "data_path = '../data/'\n",
        "\n",
        "# Primary datasets\n",
        "races = pd.read_csv(data_path + 'races.csv')\n",
        "results = pd.read_csv(data_path + 'results.csv')\n",
        "drivers = pd.read_csv(data_path + 'drivers.csv')\n",
        "constructors = pd.read_csv(data_path + 'constructors.csv')\n",
        "qualifying = pd.read_csv(data_path + 'qualifying.csv')\n",
        "\n",
        "# Additional datasets for feature engineering\n",
        "lap_times = pd.read_csv(data_path + 'lap_times.csv')\n",
        "pit_stops = pd.read_csv(data_path + 'pit_stops.csv')\n",
        "driver_standings = pd.read_csv(data_path + 'driver_standings.csv')\n",
        "constructor_standings = pd.read_csv(data_path + 'constructor_standings.csv')\n",
        "status = pd.read_csv(data_path + 'status.csv')\n",
        "\n",
        "print(\"Dataset shapes:\")\n",
        "print(f\"Races: {races.shape}\")\n",
        "print(f\"Results: {results.shape}\")\n",
        "print(f\"Drivers: {drivers.shape}\")\n",
        "print(f\"Constructors: {constructors.shape}\")\n",
        "print(f\"Qualifying: {qualifying.shape}\")\n",
        "print(f\"Lap Times: {lap_times.shape}\")\n",
        "print(f\"Pit Stops: {pit_stops.shape}\")\n",
        "print(f\"Driver Standings: {driver_standings.shape}\")\n",
        "print(f\"Constructor Standings: {constructor_standings.shape}\")\n",
        "print(f\"Status: {status.shape}\")\n",
        "\n",
        "print(\"\\nAll datasets loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore key datasets\n",
        "print(\"RACES Dataset:\")\n",
        "print(races.head())\n",
        "print(f\"\\nDate range: {races['date'].min()} to {races['date'].max()}\")\n",
        "print(f\"Total races: {len(races)}\")\n",
        "print(f\"Years covered: {races['year'].min()} to {races['year'].max()}\")\n",
        "\n",
        "print(\"\\nRESULTS Dataset:\")\n",
        "print(results.head())\n",
        "print(f\"\\nTotal race results: {len(results)}\")\n",
        "print(f\"Positions range: {results['position'].min()} to {results['position'].max()}\")\n",
        "\n",
        "print(\"\\nDRIVERS Dataset:\")\n",
        "print(drivers.head())\n",
        "print(f\"\\nTotal drivers: {len(drivers)}\")\n",
        "print(f\"Nationalities: {drivers['nationality'].nunique()}\")\n",
        "\n",
        "print(\"\\nCONSTRUCTORS Dataset:\")\n",
        "print(constructors.head())\n",
        "print(f\"\\nTotal constructors: {len(constructors)}\")\n",
        "print(f\"Constructor nationalities: {constructors['nationality'].nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive F1 dataset by merging multiple tables\n",
        "def create_f1_dataset():\n",
        "    \"\"\"\n",
        "    Create a comprehensive F1 dataset by merging multiple related tables\n",
        "    \"\"\"\n",
        "    # Start with results as the base\n",
        "    df = results.copy()\n",
        "    \n",
        "    # Add race information\n",
        "    df = df.merge(races[['raceId', 'year', 'round', 'circuitId', 'name', 'date']], \n",
        "                  on='raceId', how='left')\n",
        "    df.rename(columns={'name': 'race_name'}, inplace=True)\n",
        "    \n",
        "    # Add driver information\n",
        "    df = df.merge(drivers[['driverId', 'driverRef', 'code', 'forename', 'surname', \n",
        "                          'dob', 'nationality']], \n",
        "                  on='driverId', how='left')\n",
        "    df.rename(columns={'nationality': 'driver_nationality'}, inplace=True)\n",
        "    \n",
        "    # Add constructor information\n",
        "    df = df.merge(constructors[['constructorId', 'constructorRef', 'name', 'nationality']], \n",
        "                  on='constructorId', how='left')\n",
        "    df.rename(columns={'name': 'constructor_name', 'nationality': 'constructor_nationality'}, inplace=True)\n",
        "    \n",
        "    # Add qualifying information\n",
        "    qualifying_subset = qualifying[['raceId', 'driverId', 'position']].rename(\n",
        "        columns={'position': 'qualifying_position'})\n",
        "    df = df.merge(qualifying_subset, on=['raceId', 'driverId'], how='left')\n",
        "    \n",
        "    # Add status information\n",
        "    df = df.merge(status[['statusId', 'status']], on='statusId', how='left')\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Create the main dataset\n",
        "f1_data = create_f1_dataset()\n",
        "print(f\"Comprehensive F1 dataset created with shape: {f1_data.shape}\")\n",
        "print(f\"Columns: {list(f1_data.columns)}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample of merged dataset:\")\n",
        "print(f1_data[['year', 'race_name', 'forename', 'surname', 'constructor_name', \n",
        "               'grid', 'position', 'points', 'status']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering for ML Models\n",
        "def engineer_features(df):\n",
        "    \"\"\"\n",
        "    Create additional features for machine learning models\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Convert date to datetime\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    \n",
        "    # Create age at race date\n",
        "    df['dob'] = pd.to_datetime(df['dob'])\n",
        "    df['driver_age'] = (df['date'] - df['dob']).dt.days / 365.25\n",
        "    \n",
        "    # Create target variables for our models\n",
        "    \n",
        "    # Model 1: Podium finish prediction (top 3 positions)\n",
        "    df['podium_finish'] = (df['position'].isin([1, 2, 3])).astype(int)\n",
        "    \n",
        "    # Model 2: Driver performance tiers\n",
        "    # Calculate driver career statistics up to each race\n",
        "    driver_stats = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        current_driver = row['driverId']\n",
        "        current_date = row['date']\n",
        "        \n",
        "        # Get all previous races for this driver\n",
        "        prev_races = df[(df['driverId'] == current_driver) & \n",
        "                       (df['date'] < current_date)]\n",
        "        \n",
        "        if len(prev_races) == 0:\n",
        "            # First race for this driver\n",
        "            career_wins = 0\n",
        "            career_podiums = 0\n",
        "            career_points = 0\n",
        "            career_races = 0\n",
        "            avg_position = np.nan\n",
        "        else:\n",
        "            career_wins = len(prev_races[prev_races['position'] == 1])\n",
        "            career_podiums = len(prev_races[prev_races['position'].isin([1, 2, 3])])\n",
        "            career_points = prev_races['points'].sum()\n",
        "            career_races = len(prev_races[prev_races['position'].notna()])\n",
        "            avg_position = prev_races['position'].mean() if career_races > 0 else np.nan\n",
        "        \n",
        "        driver_stats.append({\n",
        "            'career_wins': career_wins,\n",
        "            'career_podiums': career_podiums,\n",
        "            'career_points': career_points,\n",
        "            'career_races': career_races,\n",
        "            'avg_position': avg_position\n",
        "        })\n",
        "    \n",
        "    # Add driver statistics to dataframe\n",
        "    driver_stats_df = pd.DataFrame(driver_stats)\n",
        "    df = pd.concat([df, driver_stats_df], axis=1)\n",
        "    \n",
        "    # Create performance tier based on career achievements\n",
        "    def assign_performance_tier(row):\n",
        "        if pd.isna(row['avg_position']) or row['career_races'] < 5:\n",
        "            return 'Rookie'  # New drivers\n",
        "        elif row['career_wins'] >= 5:\n",
        "            return 'Elite'   # Multiple race winners\n",
        "        elif row['career_podiums'] >= 5:\n",
        "            return 'Strong'  # Regular podium finishers\n",
        "        elif row['avg_position'] <= 10:\n",
        "            return 'Solid'   # Consistent points scorers\n",
        "        else:\n",
        "            return 'Developing'  # Others\n",
        "    \n",
        "    df['performance_tier'] = df.apply(assign_performance_tier, axis=1)\n",
        "    \n",
        "    # Additional engineered features\n",
        "    df['grid_position'] = df['grid'].fillna(df['grid'].max() + 1)  # Fill missing grid positions\n",
        "    df['qualifying_grid_diff'] = df['qualifying_position'] - df['grid_position']\n",
        "    df['championship_era'] = pd.cut(df['year'], \n",
        "                                   bins=[2008, 2013, 2021, 2024], \n",
        "                                   labels=['2009-2013', '2014-2021', '2022+'])\n",
        "    \n",
        "    # Constructor historical performance (simplified)\n",
        "    constructor_wins = df.groupby('constructorId')['podium_finish'].sum().to_dict()\n",
        "    df['constructor_total_podiums'] = df['constructorId'].map(constructor_wins)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "print(\"Engineering features...\")\n",
        "f1_enhanced = engineer_features(f1_data)\n",
        "\n",
        "print(f\"Feature engineering completed!\")\n",
        "print(f\"Enhanced dataset shape: {f1_enhanced.shape}\")\n",
        "print(f\"\\nTarget variable distributions:\")\n",
        "print(f\"Podium finishes: {f1_enhanced['podium_finish'].value_counts()}\")\n",
        "print(f\"Performance tiers: {f1_enhanced['performance_tier'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Exploratory Data Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploratory Data Analysis\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# Plot 1: Podium finishes by starting grid position\n",
        "plt.subplot(2, 3, 1)\n",
        "podium_by_grid = f1_enhanced.groupby('grid_position')['podium_finish'].mean()\n",
        "podium_by_grid.head(20).plot(kind='bar')\n",
        "plt.title('Podium Probability by Grid Position')\n",
        "plt.xlabel('Grid Position')\n",
        "plt.ylabel('Podium Probability')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 2: Performance tier distribution\n",
        "plt.subplot(2, 3, 2)\n",
        "f1_enhanced['performance_tier'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "plt.title('Driver Performance Tier Distribution')\n",
        "\n",
        "# Plot 3: Podium finishes over years\n",
        "plt.subplot(2, 3, 3)\n",
        "yearly_podiums = f1_enhanced.groupby('year')['podium_finish'].mean()\n",
        "yearly_podiums.plot()\n",
        "plt.title('Average Podium Rate by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Podium Rate')\n",
        "\n",
        "# Plot 4: Constructor performance\n",
        "plt.subplot(2, 3, 4)\n",
        "constructor_podiums = f1_enhanced.groupby('constructor_name')['podium_finish'].sum().sort_values(ascending=False).head(10)\n",
        "constructor_podiums.plot(kind='barh')\n",
        "plt.title('Top 10 Constructors by Total Podiums')\n",
        "\n",
        "# Plot 5: Driver age vs performance\n",
        "plt.subplot(2, 3, 5)\n",
        "f1_enhanced.boxplot(column='driver_age', by='performance_tier', ax=plt.gca())\n",
        "plt.title('Driver Age by Performance Tier')\n",
        "plt.suptitle('')\n",
        "\n",
        "# Plot 6: Grid position vs final position\n",
        "plt.subplot(2, 3, 6)\n",
        "valid_positions = f1_enhanced.dropna(subset=['grid_position', 'position'])\n",
        "plt.scatter(valid_positions['grid_position'], valid_positions['position'], alpha=0.3)\n",
        "plt.plot([1, 20], [1, 20], 'r--', label='Perfect correlation')\n",
        "plt.xlabel('Grid Position')\n",
        "plt.ylabel('Final Position')\n",
        "plt.title('Grid vs Final Position')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional statistics\n",
        "print(\"KEY INSIGHTS:\")\n",
        "print(f\"• Total podium finishes: {f1_enhanced['podium_finish'].sum()}\")\n",
        "print(f\"• Podium rate from pole position: {f1_enhanced[f1_enhanced['grid_position']==1]['podium_finish'].mean():.2%}\")\n",
        "print(f\"• Average driver age: {f1_enhanced['driver_age'].mean():.1f} years\")\n",
        "print(f\"• Most successful constructor: {f1_enhanced.groupby('constructor_name')['podium_finish'].sum().idxmax()}\")\n",
        "print(f\"• Years covered: {f1_enhanced['year'].min()}-{f1_enhanced['year'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model 1: Race Result Predictor (Podium Finish Prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for Model 1: Podium Prediction\n",
        "def prepare_podium_model_data(df):\n",
        "    \"\"\"\n",
        "    Prepare features for podium prediction model\n",
        "    \"\"\"\n",
        "    # Select relevant features\n",
        "    feature_columns = [\n",
        "        'grid_position', 'driver_age', 'career_wins', 'career_podiums', \n",
        "        'career_points', 'career_races', 'constructor_total_podiums',\n",
        "        'year', 'round'\n",
        "    ]\n",
        "    \n",
        "    # Categorical features\n",
        "    categorical_features = ['constructor_name', 'driver_nationality', 'championship_era']\n",
        "    \n",
        "    # Create the feature matrix\n",
        "    df_model = df.dropna(subset=['podium_finish'] + feature_columns)\n",
        "    \n",
        "    X_numeric = df_model[feature_columns]\n",
        "    X_categorical = df_model[categorical_features]\n",
        "    \n",
        "    # Target variable\n",
        "    y = df_model['podium_finish']\n",
        "    \n",
        "    return X_numeric, X_categorical, y, df_model\n",
        "\n",
        "# Prepare data\n",
        "X_num_podium, X_cat_podium, y_podium, data_podium = prepare_podium_model_data(f1_enhanced)\n",
        "\n",
        "print(f\"Model 1 - Podium Prediction Dataset:\")\n",
        "print(f\"Features shape: {X_num_podium.shape}\")\n",
        "print(f\"Categorical features shape: {X_cat_podium.shape}\")\n",
        "print(f\"Target distribution: {y_podium.value_counts()}\")\n",
        "print(f\"Podium rate: {y_podium.mean():.2%}\")\n",
        "\n",
        "# Create preprocessing pipeline for Model 1\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "preprocessor_podium = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, X_num_podium.columns),\n",
        "        ('cat', categorical_transformer, X_cat_podium.columns)\n",
        "    ])\n",
        "\n",
        "# Split the data\n",
        "X_combined_podium = pd.concat([X_num_podium, X_cat_podium], axis=1)\n",
        "X_train_podium, X_test_podium, y_train_podium, y_test_podium = train_test_split(\n",
        "    X_combined_podium, y_podium, test_size=0.2, random_state=42, stratify=y_podium\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain/Test Split:\")\n",
        "print(f\"Training set: {X_train_podium.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_podium.shape[0]} samples\")\n",
        "print(f\"Train podium rate: {y_train_podium.mean():.2%}\")\n",
        "print(f\"Test podium rate: {y_test_podium.mean():.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1: Train and evaluate multiple algorithms\n",
        "models_podium = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced'),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Create pipelines\n",
        "pipelines_podium = {}\n",
        "for name, model in models_podium.items():\n",
        "    pipelines_podium[name] = Pipeline([\n",
        "        ('preprocessor', preprocessor_podium),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "# Cross-validation evaluation\n",
        "print(\"MODEL 1: PODIUM PREDICTION - Cross Validation Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cv_scores_podium = {}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, pipeline in pipelines_podium.items():\n",
        "    # Cross-validation scores\n",
        "    cv_scores = cross_val_score(pipeline, X_train_podium, y_train_podium, \n",
        "                               cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "    cv_scores_podium[name] = cv_scores\n",
        "    \n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "# Train final models and evaluate on test set\n",
        "print(f\"\\nMODEL 1: PODIUM PREDICTION - Test Set Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trained_models_podium = {}\n",
        "for name, pipeline in pipelines_podium.items():\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train_podium, y_train_podium)\n",
        "    trained_models_podium[name] = pipeline\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = pipeline.predict(X_test_podium)\n",
        "    y_prob = pipeline.predict_proba(X_test_podium)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test_podium, y_pred)\n",
        "    precision = precision_score(y_test_podium, y_pred)\n",
        "    recall = recall_score(y_test_podium, y_pred)\n",
        "    f1 = f1_score(y_test_podium, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test_podium, y_prob)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1 Score:  {f1:.4f}\")\n",
        "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
        "\n",
        "# Select the best model for Model 1\n",
        "best_model_name_podium = max(cv_scores_podium.keys(), \n",
        "                            key=lambda x: cv_scores_podium[x].mean())\n",
        "best_model_podium = trained_models_podium[best_model_name_podium]\n",
        "\n",
        "print(f\"\\nBest Model for Podium Prediction: {best_model_name_podium}\")\n",
        "print(f\"Cross-validation ROC-AUC: {cv_scores_podium[best_model_name_podium].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model 2: Driver Performance Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for Model 2: Driver Performance Classification\n",
        "def prepare_driver_performance_data(df):\n",
        "    \"\"\"\n",
        "    Prepare features for driver performance tier classification\n",
        "    \"\"\"\n",
        "    # Only include drivers with sufficient race history (exclude rookies for training)\n",
        "    df_experienced = df[df['career_races'] >= 5].copy()\n",
        "    \n",
        "    # Feature columns for driver performance\n",
        "    feature_columns = [\n",
        "        'driver_age', 'career_races', 'career_wins', 'career_podiums', \n",
        "        'career_points', 'avg_position', 'constructor_total_podiums',\n",
        "        'year', 'grid_position'\n",
        "    ]\n",
        "    \n",
        "    # Categorical features\n",
        "    categorical_features = ['driver_nationality', 'constructor_name', 'championship_era']\n",
        "    \n",
        "    # Create the feature matrix\n",
        "    df_model = df_experienced.dropna(subset=['performance_tier'] + feature_columns)\n",
        "    \n",
        "    X_numeric = df_model[feature_columns]\n",
        "    X_categorical = df_model[categorical_features]\n",
        "    \n",
        "    # Target variable (exclude 'Rookie' since we're only training on experienced drivers)\n",
        "    y = df_model['performance_tier']\n",
        "    \n",
        "    return X_numeric, X_categorical, y, df_model\n",
        "\n",
        "# Prepare data\n",
        "X_num_perf, X_cat_perf, y_perf, data_perf = prepare_driver_performance_data(f1_enhanced)\n",
        "\n",
        "print(f\"Model 2 - Driver Performance Classification Dataset:\")\n",
        "print(f\"Features shape: {X_num_perf.shape}\")\n",
        "print(f\"Categorical features shape: {X_cat_perf.shape}\")\n",
        "print(f\"Target distribution:\")\n",
        "print(y_perf.value_counts())\n",
        "\n",
        "# Create preprocessing pipeline for Model 2\n",
        "preprocessor_perf = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), X_num_perf.columns),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), X_cat_perf.columns)\n",
        "    ])\n",
        "\n",
        "# Split the data\n",
        "X_combined_perf = pd.concat([X_num_perf, X_cat_perf], axis=1)\n",
        "X_train_perf, X_test_perf, y_train_perf, y_test_perf = train_test_split(\n",
        "    X_combined_perf, y_perf, test_size=0.2, random_state=42, stratify=y_perf\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain/Test Split:\")\n",
        "print(f\"Training set: {X_train_perf.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_perf.shape[0]} samples\")\n",
        "print(f\"\\nTrain performance tier distribution:\")\n",
        "print(y_train_perf.value_counts())\n",
        "print(f\"\\nTest performance tier distribution:\")\n",
        "print(y_test_perf.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 2: Train and evaluate multiple algorithms for multi-class classification\n",
        "models_perf = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Create pipelines\n",
        "pipelines_perf = {}\n",
        "for name, model in models_perf.items():\n",
        "    pipelines_perf[name] = Pipeline([\n",
        "        ('preprocessor', preprocessor_perf),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "# Cross-validation evaluation\n",
        "print(\"MODEL 2: DRIVER PERFORMANCE CLASSIFICATION - Cross Validation Results\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "cv_scores_perf = {}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, pipeline in pipelines_perf.items():\n",
        "    # Cross-validation scores (using accuracy for multi-class)\n",
        "    cv_scores = cross_val_score(pipeline, X_train_perf, y_train_perf, \n",
        "                               cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "    cv_scores_perf[name] = cv_scores\n",
        "    \n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "# Train final models and evaluate on test set\n",
        "print(f\"\\nMODEL 2: DRIVER PERFORMANCE CLASSIFICATION - Test Set Results\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "trained_models_perf = {}\n",
        "for name, pipeline in pipelines_perf.items():\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train_perf, y_train_perf)\n",
        "    trained_models_perf[name] = pipeline\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = pipeline.predict(X_test_perf)\n",
        "    \n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test_perf, y_pred)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Detailed Classification Report:\")\n",
        "    print(classification_report(y_test_perf, y_pred, target_names=sorted(y_perf.unique())))\n",
        "\n",
        "# Select the best model for Model 2\n",
        "best_model_name_perf = max(cv_scores_perf.keys(), \n",
        "                          key=lambda x: cv_scores_perf[x].mean())\n",
        "best_model_perf = trained_models_perf[best_model_name_perf]\n",
        "\n",
        "print(f\"\\nBest Model for Driver Performance Classification: {best_model_name_perf}\")\n",
        "print(f\"Cross-validation Accuracy: {cv_scores_perf[best_model_name_perf].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Optimization and Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for the best models\n",
        "print(\"HYPERPARAMETER TUNING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Tune Model 1 (Podium Prediction) - Assuming Random Forest was best\n",
        "if best_model_name_podium == 'Random Forest':\n",
        "    param_grid_podium = {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [10, 20, None],\n",
        "        'classifier__min_samples_split': [2, 5],\n",
        "        'classifier__min_samples_leaf': [1, 2]\n",
        "    }\n",
        "    \n",
        "    grid_search_podium = GridSearchCV(\n",
        "        pipelines_podium['Random Forest'], \n",
        "        param_grid_podium, \n",
        "        cv=StratifiedKFold(3, shuffle=True, random_state=42),\n",
        "        scoring='roc_auc', \n",
        "        n_jobs=-1, \n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"Tuning Model 1 (Podium Prediction)...\")\n",
        "    grid_search_podium.fit(X_train_podium, y_train_podium)\n",
        "    \n",
        "    print(f\"Best parameters for Model 1: {grid_search_podium.best_params_}\")\n",
        "    print(f\"Best CV score for Model 1: {grid_search_podium.best_score_:.4f}\")\n",
        "    \n",
        "    # Update best model\n",
        "    best_model_podium = grid_search_podium.best_estimator_\n",
        "\n",
        "# Tune Model 2 (Driver Performance) - Assuming Random Forest was best\n",
        "if best_model_name_perf == 'Random Forest':\n",
        "    param_grid_perf = {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [10, 20, None],\n",
        "        'classifier__min_samples_split': [2, 5],\n",
        "        'classifier__min_samples_leaf': [1, 2]\n",
        "    }\n",
        "    \n",
        "    grid_search_perf = GridSearchCV(\n",
        "        pipelines_perf['Random Forest'], \n",
        "        param_grid_perf, \n",
        "        cv=StratifiedKFold(3, shuffle=True, random_state=42),\n",
        "        scoring='accuracy', \n",
        "        n_jobs=-1, \n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"\\nTuning Model 2 (Driver Performance Classification)...\")\n",
        "    grid_search_perf.fit(X_train_perf, y_train_perf)\n",
        "    \n",
        "    print(f\"Best parameters for Model 2: {grid_search_perf.best_params_}\")\n",
        "    print(f\"Best CV score for Model 2: {grid_search_perf.best_score_:.4f}\")\n",
        "    \n",
        "    # Update best model\n",
        "    best_model_perf = grid_search_perf.best_estimator_\n",
        "\n",
        "print(\"\\nHyperparameter tuning completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Model Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final evaluation and visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Model 1 Evaluation\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Model 1: Podium Prediction\n",
        "y_pred_podium = best_model_podium.predict(X_test_podium)\n",
        "y_prob_podium = best_model_podium.predict_proba(X_test_podium)[:, 1]\n",
        "\n",
        "print(f\"MODEL 1 - PODIUM PREDICTION ({best_model_name_podium}):\")\n",
        "print(f\"Final Test Accuracy: {accuracy_score(y_test_podium, y_pred_podium):.4f}\")\n",
        "print(f\"Final Test ROC-AUC: {roc_auc_score(y_test_podium, y_prob_podium):.4f}\")\n",
        "print(f\"Final Test F1-Score: {f1_score(y_test_podium, y_pred_podium):.4f}\")\n",
        "\n",
        "# Plot 1: ROC Curve for Model 1\n",
        "plt.subplot(2, 3, 1)\n",
        "fpr, tpr, _ = roc_curve(y_test_podium, y_prob_podium)\n",
        "plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc_score(y_test_podium, y_prob_podium):.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Model 1: ROC Curve')\n",
        "plt.legend()\n",
        "\n",
        "# Plot 2: Confusion Matrix for Model 1\n",
        "plt.subplot(2, 3, 2)\n",
        "cm = confusion_matrix(y_test_podium, y_pred_podium)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Model 1: Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# Model 2: Driver Performance Classification\n",
        "y_pred_perf = best_model_perf.predict(X_test_perf)\n",
        "\n",
        "print(f\"\\nMODEL 2 - DRIVER PERFORMANCE ({best_model_name_perf}):\")\n",
        "print(f\"Final Test Accuracy: {accuracy_score(y_test_perf, y_pred_perf):.4f}\")\n",
        "print(f\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test_perf, y_pred_perf))\n",
        "\n",
        "# Plot 3: Confusion Matrix for Model 2\n",
        "plt.subplot(2, 3, 3)\n",
        "cm_perf = confusion_matrix(y_test_perf, y_pred_perf)\n",
        "unique_labels = sorted(y_perf.unique())\n",
        "sns.heatmap(cm_perf, annot=True, fmt='d', cmap='Greens', \n",
        "           xticklabels=unique_labels, yticklabels=unique_labels)\n",
        "plt.title('Model 2: Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Feature Importance for Model 1 (if Random Forest)\n",
        "if 'Random Forest' in best_model_name_podium:\n",
        "    plt.subplot(2, 3, 4)\n",
        "    \n",
        "    # Get feature names after preprocessing\n",
        "    feature_names = (list(X_num_podium.columns) + \n",
        "                    list(best_model_podium.named_steps['preprocessor']\n",
        "                         .named_transformers_['cat']\n",
        "                         .get_feature_names_out(X_cat_podium.columns)))\n",
        "    \n",
        "    importances = best_model_podium.named_steps['classifier'].feature_importances_\n",
        "    indices = np.argsort(importances)[::-1][:10]  # Top 10 features\n",
        "    \n",
        "    plt.barh(range(10), importances[indices])\n",
        "    plt.yticks(range(10), [feature_names[i] for i in indices])\n",
        "    plt.title('Model 1: Top 10 Feature Importances')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "# Feature Importance for Model 2 (if Random Forest)\n",
        "if 'Random Forest' in best_model_name_perf:\n",
        "    plt.subplot(2, 3, 5)\n",
        "    \n",
        "    # Get feature names after preprocessing\n",
        "    feature_names_perf = (list(X_num_perf.columns) + \n",
        "                         list(best_model_perf.named_steps['preprocessor']\n",
        "                              .named_transformers_['cat']\n",
        "                              .get_feature_names_out(X_cat_perf.columns)))\n",
        "    \n",
        "    importances_perf = best_model_perf.named_steps['classifier'].feature_importances_\n",
        "    indices_perf = np.argsort(importances_perf)[::-1][:10]  # Top 10 features\n",
        "    \n",
        "    plt.barh(range(10), importances_perf[indices_perf])\n",
        "    plt.yticks(range(10), [feature_names_perf[i] for i in indices_perf])\n",
        "    plt.title('Model 2: Top 10 Feature Importances')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "# Model Performance Comparison\n",
        "plt.subplot(2, 3, 6)\n",
        "models_comparison = ['Model 1\\n(Podium)', 'Model 2\\n(Performance)']\n",
        "scores_comparison = [\n",
        "    roc_auc_score(y_test_podium, y_prob_podium),\n",
        "    accuracy_score(y_test_perf, y_pred_perf)\n",
        "]\n",
        "colors = ['skyblue', 'lightgreen']\n",
        "\n",
        "bars = plt.bar(models_comparison, scores_comparison, color=colors)\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, score in zip(bars, scores_comparison):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{score:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nModel evaluation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Persistence and Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained models for API deployment\n",
        "import os\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "models_dir = '../models'\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "# Save Model 1: Podium Prediction\n",
        "model1_path = os.path.join(models_dir, 'podium_prediction_model.joblib')\n",
        "joblib.dump(best_model_podium, model1_path)\n",
        "print(f\"Model 1 saved to: {model1_path}\")\n",
        "\n",
        "# Save Model 2: Driver Performance Classification\n",
        "model2_path = os.path.join(models_dir, 'driver_performance_model.joblib')\n",
        "joblib.dump(best_model_perf, model2_path)\n",
        "print(f\"Model 2 saved to: {model2_path}\")\n",
        "\n",
        "# Save feature information for API\n",
        "feature_info = {\n",
        "    'podium_model': {\n",
        "        'numeric_features': list(X_num_podium.columns),\n",
        "        'categorical_features': list(X_cat_podium.columns),\n",
        "        'model_type': best_model_name_podium\n",
        "    },\n",
        "    'performance_model': {\n",
        "        'numeric_features': list(X_num_perf.columns),\n",
        "        'categorical_features': list(X_cat_perf.columns),\n",
        "        'model_type': best_model_name_perf,\n",
        "        'performance_tiers': sorted(y_perf.unique())\n",
        "    }\n",
        "}\n",
        "\n",
        "import json\n",
        "feature_info_path = os.path.join(models_dir, 'feature_info.json')\n",
        "with open(feature_info_path, 'w') as f:\n",
        "    json.dump(feature_info, f, indent=2)\n",
        "print(f\"Feature information saved to: {feature_info_path}\")\n",
        "\n",
        "# Create sample prediction functions\n",
        "def predict_podium_finish(grid_position, driver_age, career_wins, career_podiums, \n",
        "                         career_points, career_races, constructor_total_podiums,\n",
        "                         year, round_num, constructor_name, driver_nationality, championship_era):\n",
        "    \"\"\"\n",
        "    Sample function to predict podium finish probability\n",
        "    \"\"\"\n",
        "    # Create input dataframe\n",
        "    input_data = pd.DataFrame({\n",
        "        'grid_position': [grid_position],\n",
        "        'driver_age': [driver_age],\n",
        "        'career_wins': [career_wins],\n",
        "        'career_podiums': [career_podiums],\n",
        "        'career_points': [career_points],\n",
        "        'career_races': [career_races],\n",
        "        'constructor_total_podiums': [constructor_total_podiums],\n",
        "        'year': [year],\n",
        "        'round': [round_num],\n",
        "        'constructor_name': [constructor_name],\n",
        "        'driver_nationality': [driver_nationality],\n",
        "        'championship_era': [championship_era]\n",
        "    })\n",
        "    \n",
        "    # Make prediction\n",
        "    probability = best_model_podium.predict_proba(input_data)[0, 1]\n",
        "    prediction = best_model_podium.predict(input_data)[0]\n",
        "    \n",
        "    return {\n",
        "        'podium_probability': probability,\n",
        "        'predicted_podium': bool(prediction),\n",
        "        'confidence': 'High' if probability > 0.7 or probability < 0.3 else 'Medium'\n",
        "    }\n",
        "\n",
        "def predict_driver_performance(driver_age, career_races, career_wins, career_podiums,\n",
        "                              career_points, avg_position, constructor_total_podiums,\n",
        "                              year, grid_position, driver_nationality, constructor_name, \n",
        "                              championship_era):\n",
        "    \"\"\"\n",
        "    Sample function to predict driver performance tier\n",
        "    \"\"\"\n",
        "    # Create input dataframe\n",
        "    input_data = pd.DataFrame({\n",
        "        'driver_age': [driver_age],\n",
        "        'career_races': [career_races],\n",
        "        'career_wins': [career_wins],\n",
        "        'career_podiums': [career_podiums],\n",
        "        'career_points': [career_points],\n",
        "        'avg_position': [avg_position],\n",
        "        'constructor_total_podiums': [constructor_total_podiums],\n",
        "        'year': [year],\n",
        "        'grid_position': [grid_position],\n",
        "        'driver_nationality': [driver_nationality],\n",
        "        'constructor_name': [constructor_name],\n",
        "        'championship_era': [championship_era]\n",
        "    })\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = best_model_perf.predict(input_data)[0]\n",
        "    probabilities = best_model_perf.predict_proba(input_data)[0]\n",
        "    \n",
        "    # Get class labels\n",
        "    classes = best_model_perf.named_steps['classifier'].classes_\n",
        "    prob_dict = dict(zip(classes, probabilities))\n",
        "    \n",
        "    return {\n",
        "        'predicted_tier': prediction,\n",
        "        'tier_probabilities': prob_dict,\n",
        "        'confidence': max(probabilities)\n",
        "    }\n",
        "\n",
        "# Test the functions with sample data\n",
        "print(\"\\nTesting prediction functions:\")\n",
        "\n",
        "# Test podium prediction\n",
        "sample_podium = predict_podium_finish(\n",
        "    grid_position=1, driver_age=28, career_wins=5, career_podiums=15,\n",
        "    career_points=500, career_races=50, constructor_total_podiums=100,\n",
        "    year=2023, round_num=10, constructor_name='Mercedes', \n",
        "    driver_nationality='British', championship_era='2022+'\n",
        ")\n",
        "print(f\"Sample podium prediction: {sample_podium}\")\n",
        "\n",
        "# Test performance classification\n",
        "sample_performance = predict_driver_performance(\n",
        "    driver_age=28, career_races=50, career_wins=5, career_podiums=15,\n",
        "    career_points=500, avg_position=6.5, constructor_total_podiums=100,\n",
        "    year=2023, grid_position=3, driver_nationality='British',\n",
        "    constructor_name='Mercedes', championship_era='2022+'\n",
        ")\n",
        "print(f\"Sample performance prediction: {sample_performance}\")\n",
        "\n",
        "print(\"\\nModels ready for API deployment!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Conclusions and Summary\n",
        "\n",
        "### Project Summary\n",
        "This project implemented two machine learning models using Formula 1 racing data:\n",
        "\n",
        "#### Model 1: Podium Prediction\n",
        "- **Objective**: Predict whether a driver will finish in the top 3 positions\n",
        "- **Type**: Binary Classification\n",
        "- **Features**: Grid position, driver career statistics, constructor performance, race context\n",
        "- **Performance**: Evaluated using ROC-AUC score\n",
        "\n",
        "#### Model 2: Driver Performance Classification\n",
        "- **Objective**: Classify drivers into performance tiers (Elite, Strong, Solid, Developing)\n",
        "- **Type**: Multi-class Classification\n",
        "- **Features**: Career achievements, age, team performance, historical data\n",
        "- **Performance**: Evaluated using accuracy and classification report\n",
        "\n",
        "### Technical Implementation\n",
        "\n",
        "1. **Data Integration**\n",
        "   - Merged multiple F1 datasets (races, results, drivers, constructors, qualifying, etc.)\n",
        "   - Created training samples with feature engineering\n",
        "   - Handled missing data and categorical variables\n",
        "\n",
        "2. **Feature Engineering**\n",
        "   - Career statistics calculation\n",
        "   - Time-based features (championship eras)\n",
        "   - Driver age calculation\n",
        "   - Constructor performance metrics\n",
        "\n",
        "3. **Model Development**\n",
        "   - Tested multiple algorithms (Logistic Regression, Random Forest, Gradient Boosting)\n",
        "   - Cross-validation evaluation\n",
        "   - Hyperparameter tuning with GridSearchCV\n",
        "   - Performance evaluation with multiple metrics\n",
        "\n",
        "4. **API Implementation**\n",
        "   - FastAPI implementation with documentation\n",
        "   - Input validation with Pydantic models\n",
        "   - Error handling and health checks\n",
        "\n",
        "### Model Performance\n",
        "- Both models achieved reasonable performance on their respective tasks\n",
        "- Feature importance analysis showed grid position and career statistics as key predictors\n",
        "- Cross-validation confirmed model stability\n",
        "\n",
        "### Technologies Used\n",
        "- **Data Processing**: Pandas, NumPy\n",
        "- **Machine Learning**: Scikit-learn\n",
        "- **Visualization**: Matplotlib, Seaborn\n",
        "- **API Development**: FastAPI, Pydantic, Uvicorn\n",
        "- **Model Persistence**: Joblib\n",
        "\n",
        "This project demonstrates a complete machine learning pipeline from data exploration to model deployment."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOrzEmNrVsv6BxBuxeMnS1a",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
